---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
    .special-link,
    .special-link:visited,
    .special-link:hover,
    .special-link:active {
      color: inherit;
      text-decoration: none !important;
    }
  
    .teaser {
      width: 200px;
    }
  
    .collapsible-content {
      padding: 0 18px;
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.2s ease-out;
      background-color: #f1f1f1;
    }
  
    .p-bibtex {
      font-size: 12px;
      margin-bottom: 0px;
      max-width: auto;
      text-align: left;
    }
  
    .paper-entry,
    .project-entry {
      display: inline-block;
      max-width: 100%;
    }
  </style>

<p>
  I am a PhD student at the Robotic Interactive Perception Lab with 
  <a href="https://sites.google.com/view/guillermogallego">Guillermo Gallego</a>,
  affiliated with Technical University Berlin and the research center <a href="https://www.scienceofintelligence.de/">SCIoI</a>.
  I also spent part of my PhD at <a href="https://www.grasp.upenn.edu/"> UPenn GRASP lab</a> with Professor <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>.
  Previously, I did my Master's in Electrical Engineering, Information Technology, and Computer Engineering at RWTH Aachen University.
</p>
<p>
  My research interests are in computer vision and robotic perception. I work on motion estimation and scene understanding of highly dynamic environments. My research projects range from fundamental motion estimation problems with event cameras to tracking and computational photography.
</p>

<h2>News</h2>

<ul>
  <li>
    <b>March 2025:</b> The exhibition <a href="https://www.maxgoelitz.com/en/news/110-exhibition-sindelfingen-de-die-erste-institutionelle-einzelausstellung-von-justin-urbach-eroffnet/">BLINDHAED</a> opened, showing visual installations that make use of event camera data. I was very happy to support Justin, Alex and William with the realization.
  </li>
</ul>

<ul>
  <li>
    <b>Feb 2025:</b> 1 paper (<a href="https://arxiv.org/pdf/2412.00133">ETAP</a>) accepted at CVPR25.
  </li>
</ul>

<ul>
  <li>
    <b>Feb 2025:</b> We started the <a href="https://www.codabench.org/competitions/5600/">SIS Challenge</a>. Results will be presented at the <a href="https://tub-rip.github.io/eventvision2025/">CVPR 25 workshop on Event-based Vision</a>.
  </li>
</ul>

<ul>
  <li>
    <b>Oct 2024:</b> We are organizing <a href="https://moseskonto.tu-berlin.de/moses/modultransfersystem/bolognamodule/beschreibung/anzeigen.html?nummer=41200&version=1&sprache=2"> a seminar this semester (WS24/25) on vision-based tracking and motion estimation</a>. Mo 10:15, MAR 2.057.
  </li>
</ul>

<h2>Publications</h2>

<div class="paper-entry">
  <a href="https://arxiv.org/pdf/2412.00133" class="special-link">
 <p>
 <img class="align-left teaser" style="padding-bottom: 0px;" src="../images/spinner_pred.gif">
 <b>Event-based Tracking of Any Point with Motion-Robust Correlation Features.</b>
 </br>
 <u>Friedhelm Hamann</u>, Daniel Gehrig, Filbert Febryanto, Kostas Daniilidis, Guillermo Gallego
 </br>
 <em>IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2025</em>
 </br>
 <a class="btn btn--research" href="https://arxiv.org/pdf/2412.00133">
 <span class="icon" style="margin-right: 0em;">
 <img src="../images/Adobe_PDF_icon.svg" width="15"/>
 </span>
 <span>Paper</span>
 </a>
 </p>
 </a>
 </div>

<div class="paper-entry">
  <a href="https://arxiv.org/pdf/2407.10802" class="special-link">
    <p>
      <img class="align-left teaser" style="padding-bottom: 30px;" src="../images/motionpriorcm.png">
      <b>Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation.</b>
      </br>
      <u>Friedhelm Hamann</u>, Ziyun Wang, Ioannis Asmanis, Kenneth Chaney, Guillermo Gallego, Kostas Daniilidis
      </br>
      <em>European Conference on Computer Vision (ECCV), 2024</em>
      </br>
      <a class="btn btn--research" href="https://github.com/tub-rip/MotionPriorCMax">
        <span class="icon" style="margin-right: 0em;">
          <i class="fas fa-bolt"></i>
        </span>
        <span>Project Page</span>
      </a>
      <a class="btn btn--research" href="https://arxiv.org/pdf/2407.10802">
        <span class="icon" style="margin-right: 0em;">
          <img src="../images/Adobe_PDF_icon.svg" width="15"/>
        </span>
        <span>Paper</span>
      </a>
      <a class="btn btn--research" href="https://github.com/tub-rip/MotionPriorCMax">
        <span class="icon" style="margin-right: 0em;">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </p>
  </a>
</div>

<div class="paper-entry"></div>
  <a href="https://arxiv.org/pdf/2409.03358" class="special-link">
    <p>
      <img class="align-left teaser" style="padding-bottom: 30px;" src="../images/mouse_sis.gif">
      <b>MouseSIS: A Frames-and-Events Dataset for Space-Time Instance Segmentation of Mice.</b>
      </br>
      <u>Friedhelm Hamann</u>, Hanxiong Li, Paul Mieske, Lars Lewejohann, Guillermo Gallego
      </br>
      <em>ECCV Workshop on Neuromorphic Vision 2024</em>
      </br>
      <a class="btn btn--research" href="https://github.com/tub-rip/MouseSIS">
        <span class="icon" style="margin-right: 0em;">
          <i class="fas fa-bolt"></i>
        </span>
        <span>Project Page</span>
      </a>
      <a class="btn btn--research" href="https://arxiv.org/pdf/2409.03358">
        <span class="icon" style="margin-right: 0em;">
          <img src="../images/Adobe_PDF_icon.svg" width="15"/>
        </span>
        <span>Paper</span>
      </a>
      <a class="btn btn--research" href="https://github.com/tub-rip/MouseSIS">
        <span class="icon" style="margin-right: 0em;">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </p>
  </a>
</div>

<div class="paper-entry">
  <a href="https://arxiv.org/pdf/2312.03799.pdf" class="special-link">
    <p>
      <img class="align-left teaser" style="padding-bottom: 0px;" src="../images/penguins.gif">
      <b>Low-power, Continuous Remote Behavioral Localization with Event Cameras.</b>
      </br>
      <u>Friedhelm Hamann</u>, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego
      </br>
      <em>IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024</em>
      </br>
      <a class="btn btn--research" href="https://tub-rip.github.io/eventpenguins/">
        <span class="icon" style="margin-right: 0em;">
          <i class="fas fa-bolt"></i>
        </span>
        <span>Project Page</span>
      </a>
      <a class="btn btn--research" href="https://arxiv.org/pdf/2312.03799.pdf">
        <span class="icon" style="margin-right: 0em;">
          <img src="../images/Adobe_PDF_icon.svg" width="15"/>
        </span>
        <span>Paper</span>
      </a>
      <a class="btn btn--research" href="https://github.com/tub-rip/event_penguins">
        <span class="icon" style="margin-right: 0em;">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
      
    </p>
  </a>
</div>

<div class="paper-entry">
  <a href="https://arxiv.org/pdf/2312.00113.pdf" class="special-link">
    <p>
      <img class="align-left teaser" style="padding-bottom: 30px;" src="../images/continuity_cam.gif">
      <b>Event-based Continuous Color Video Decompression from Single Frames.</b>
      </br>
      Ziyun Wang, <u>Friedhelm Hamann</u>, Kenneth Chaney, Wen Jiang, Guillermo Gallego, Kostas Daniilidis
      <!--</br>
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023</em>-->
      </br>
      <a class="btn btn--research" href="https://www.cis.upenn.edu/~ziyunw/continuity_cam/">
        <span class="icon" style="margin-right: 0em;">
          <i class="fas fa-bolt"></i>
        </span>
        <span>Project Page</span>
      </a>
      <a class="btn btn--research" href="https://arxiv.org/pdf/2312.00113.pdf">
        <span class="icon" style="margin-right: 0em;">
          <img src="../images/Adobe_PDF_icon.svg" width="15"/>
        </span>
        <span>Paper</span>
      </a>
    </p>
  </a>
</div>

<div class="paper-entry">
  <a href="https://doi.org/10.1109/TPAMI.2023.3328188" class="special-link">
    <p>
      <img class="align-left teaser" style="padding-bottom: 60px;" src="../images/bos_teaser.png">
      <b>Event-based Background-Oriented Schlieren.</b>
      </br>
      Shintaro Shiba, <u>Friedhelm Hamann</u>, Yoshimitsu Aoki, Guillermo Gallego
      </br>
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023</em>
      </br>
      <a class="btn btn--research" href="https://doi.org/10.1109/TPAMI.2023.3328188">
        <span class="icon" style="margin-right: 0em;">
          <img src="../images/Adobe_PDF_icon.svg" width="15"/>
        </span>
        <span>Paper</span>
      </a>
      <a class="btn btn--research" href="https://github.com/tub-rip/event_based_bos">
        <span class="icon" style="margin-right: 0em;">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </p>
  </a>
</div>

<div class="paper-entry">
  <a href="https://homepages.inf.ed.ac.uk/rbf/VAIB22PAPERS/vaib22fhgg.pdf" class="special-link">
    <p>
      <img class="align-left teaser" style="padding-bottom: 30px;" src="../images/icprw22crop.jpg">
      <b>Stereo Co-capture System for Recording and Tracking Fish with Frame-and Event Cameras.</b>
      </br>
      <u>Friedhelm Hamann</u>, Guillermo Gallego
      </br>
      <em>26th Int. Conf. Pattern Recognition (ICPR), Visual observation and analysis workshop, 2022</em>
      </br>
      <a class="btn btn--research" href="https://homepages.inf.ed.ac.uk/rbf/VAIB22PAPERS/vaib22fhgg.pdf">
        <span class="icon" style="margin-right: 0em;">
          <img src="../images/Adobe_PDF_icon.svg" width="15"/>
        </span>
        <span>Paper</span>
      </a>
    </p>
  </a>
</div>


<h2>Projects</h2>


<div class="project-entry">
  <a href="https://github.com/tub-rip/CoCapture" class="special-link">
    <img class="align-left teaser" src="../images/cocapture_example.png">
    <b>CoCapture</b>
    </br>
    GUI for viewing and recording with multi camera systems including event cameras. 
  </a>
</div>
